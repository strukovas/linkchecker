{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import selenium\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait, Select\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sitemap(url):\n",
    "    get_url = requests.get(url)\n",
    "\n",
    "    if get_url.status_code == 200:\n",
    "        return get_url.text\n",
    "    else:\n",
    "        print ('Unable to fetch sitemap: %s.' % url)\n",
    "\n",
    "\n",
    "def process_sitemap(s):\n",
    "    soup = BeautifulSoup(s, \"lxml\")\n",
    "    result = []\n",
    "\n",
    "    for loc in soup.findAll(\"loc\"):\n",
    "        result.append(loc.text)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def is_sub_sitemap(s):\n",
    "    if s.endswith('.xml') and 'sitemap' in s:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "\n",
    "def parse_sitemap(s):\n",
    "    sitemap = process_sitemap(s)\n",
    "    result = []\n",
    "\n",
    "    while sitemap:\n",
    "        candidate = sitemap.pop()\n",
    "\n",
    "        if is_sub_sitemap(candidate):\n",
    "            sub_sitemap = get_sitemap(candidate)\n",
    "            for i in process_sitemap(sub_sitemap):\n",
    "                sitemap.append(i)\n",
    "        else:\n",
    "            result.append(candidate)\n",
    "\n",
    "    return result\n",
    "\n",
    "def only_articles(a):\n",
    "    only_articles = []\n",
    "    \n",
    "    for elem in a:\n",
    "        if re.search(r\"article\", elem):\n",
    "            only_articles.append(elem)\n",
    "    return only_articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXCLUDED_URLS = ['https://www.example.com', 'https://www.example.com']\n",
    "\n",
    "sitemap = get_sitemap('https://www.example.com/sitemap.xml')\n",
    "result = parse_sitemap(sitemap)\n",
    "articles = only_articles(result)\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "#agent = driver.execute_script(\"return navigator.userAgent\")\n",
    "driver.set_page_load_timeout(20)\n",
    "#print(\"User agent: \"+agent)\n",
    "with open('linkchecker.csv', mode='w', encoding=\"utf-8\") as outfile:\n",
    "        writer = csv.writer(outfile, quoting=csv.QUOTE_ALL)\n",
    "        header_row = [\"article_url\", \"link_text\", \"link_url\", \"status_code\", \"final_url\"]\n",
    "        writer.writerow(header_row)\n",
    "        try:\n",
    "            for article in articles:\n",
    "                driver.get(article)\n",
    "                print('ARTICLE:', article)\n",
    "                driver.wait = WebDriverWait(driver, 2)\n",
    "                driver.execute_script(\"if (document.body !== null) {window.scrollTo(0, 900);}\")\n",
    "                time.sleep(1)\n",
    "                #bodytext = driver.find_element_by_tag_name(\"body\").text\n",
    "                bodyhtml = driver.find_element_by_tag_name(\"body\").get_attribute('outerHTML')\n",
    "                #print(bodyhtml)\n",
    "                soup = BeautifulSoup(bodyhtml, \"html.parser\")\n",
    "                links = soup.findAll(\"a[not(starts-with(@href,'../'))]\")\n",
    "                links = driver.find_elements_by_xpath(\"//a[starts-with(@href, 'http') and not(contains(@href, 'www.example.com'))]\")\n",
    "                print(\"We had \"+str(len(links))+\" ducks to feed\")\n",
    "\n",
    "                selected_urls = []\n",
    "                selected_urls_text = []\n",
    "                ducks = 0\n",
    "                for link in links:\n",
    "                    href = link.get_attribute(\"href\")\n",
    "                    if link.get_attribute(\"href\") not in EXCLUDED_URLS:\n",
    "                        selected_urls.append(href)\n",
    "                        selected_urls_text.append(link.text)\n",
    "                        ducks = ducks + 1\n",
    "                print(\"We have \"+str(ducks)+\" ducks to feed\\n\")\n",
    "\n",
    "                for url, text in zip(selected_urls, selected_urls_text):\n",
    "                    print(\"Retrieving : \"+ url)\n",
    "                    print(\"Anchor text: \"+ text)\n",
    "                    row = [article, text, url, \"\", \"\"]\n",
    "                    try:\n",
    "                        driver.get(url)\n",
    "                        print(\"Final url: \"+ driver.current_url)\n",
    "                        r = requests.head(url)\n",
    "                        print(f\"HTTP code: {r.status_code}\\n\")\n",
    "                        row = [article, text, url, r.status_code, driver.current_url]\n",
    "                        # prints the int of the status code\n",
    "                    except requests.ConnectionError as e:\n",
    "                        print(\"failed to connect\")\n",
    "                    except TimeoutException as e:\n",
    "                        print(\"ERORR: TimeoutException\")\n",
    "                        # Add \"TIMEOUT\" instead of HTTP status code\n",
    "                        pass\n",
    "                    except (WebDriverException) as e:\n",
    "                        print(\"Hello! I hate seagulls!\")\n",
    "                        print(e)\n",
    "                        pass\n",
    "                    writer.writerow(row)\n",
    "\n",
    "        except (WebDriverException) as e:\n",
    "            print(\"Hello I love ducks! ðŸ¦†\") # I really do love ducks\n",
    "            print(e)\n",
    "            pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
